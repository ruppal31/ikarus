{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68b274c9-44ff-4c0e-8365-39cfbc722688",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/dill-0.3.9-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/texttable-1.7.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/nvfuser-0.2.13a0+0d33366-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/igraph-0.11.8-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/opt_einsum-3.4.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_utilities-0.11.9-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.0.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/looseversion-1.3.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (1.26.4)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.2.2+cu121)\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (2.6.1)\n",
      "Collecting pinecone-client\n",
      "  Downloading pinecone_client-6.0.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.12/dist-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.9.86)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.14.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.35.3)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.0.0)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.12/dist-packages (from pinecone-client) (2024.8.30)\n",
      "Collecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone-client)\n",
      "  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: urllib3>=1.26.5 in /usr/local/lib/python3.12/dist-packages (from pinecone-client) (2.0.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (1.1.10)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch) (1.3.0)\n",
      "Downloading pinecone_client-6.0.0-py3-none-any.whl (6.7 kB)\n",
      "Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\n",
      "Installing collected packages: pinecone-plugin-interface, pinecone-client\n",
      "Successfully installed pinecone-client-6.0.0 pinecone-plugin-interface-0.0.7\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy torch sentence-transformers pinecone-client transformers tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d49bf714-21e1-45f6-befa-cd1428c5394c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/dill-0.3.9-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/texttable-1.7.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/nvfuser-0.2.13a0+0d33366-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/igraph-0.11.8-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/opt_einsum-3.4.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_utilities-0.11.9-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.0.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/looseversion-1.3.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting pinecone\n",
      "  Downloading pinecone-7.3.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.12/dist-packages (from pinecone) (2024.8.30)\n",
      "Collecting pinecone-plugin-assistant<2.0.0,>=1.6.0 (from pinecone)\n",
      "  Downloading pinecone_plugin_assistant-1.8.0-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.12/dist-packages (from pinecone) (0.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from pinecone) (2.9.0.post0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.12/dist-packages (from pinecone) (4.12.2)\n",
      "Requirement already satisfied: urllib3>=1.26.5 in /usr/local/lib/python3.12/dist-packages (from pinecone) (2.0.7)\n",
      "Collecting packaging<25.0,>=24.2 (from pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone)\n",
      "  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.3 in /usr/local/lib/python3.12/dist-packages (from pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (2.32.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.5.3->pinecone) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.3->pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.3->pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (3.10)\n",
      "Downloading pinecone-7.3.0-py3-none-any.whl (587 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.6/587.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Downloading pinecone_plugin_assistant-1.8.0-py3-none-any.whl (259 kB)\n",
      "Downloading packaging-24.2-py3-none-any.whl (65 kB)\n",
      "Installing collected packages: packaging, pinecone-plugin-assistant, pinecone\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 23.2\n",
      "    Uninstalling packaging-23.2:\n",
      "      Successfully uninstalled packaging-23.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torch-tensorrt 2.6.0a0 requires torch>=2.5.0, but you have torch 2.2.2+cu121 which is incompatible.\n",
      "nvidia-dali-cuda120 1.44.0 requires six<=1.16,>=1.16, but you have six 1.17.0 which is incompatible.\n",
      "lightning-thunder 0.2.0.dev0 requires torch>=2.3.0, but you have torch 2.2.2+cu121 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed packaging-24.2 pinecone-7.3.0 pinecone-plugin-assistant-1.8.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "init is no longer a top-level attribute of the pinecone package.\n\nPlease create an instance of the Pinecone class instead.\n\nExample:\n\n    import os\n    from pinecone import Pinecone, ServerlessSpec\n\n    pc = Pinecone(\n        api_key=os.environ.get(\"PINECONE_API_KEY\")\n    )\n\n    # Now do stuff\n    if 'my_index' not in pc.list_indexes().names():\n        pc.create_index(\n            name='my_index',\n            dimension=1536,\n            metric='euclidean',\n            spec=ServerlessSpec(\n                cloud='aws',\n                region='us-west-2'\n            )\n        )\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install pinecone\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpinecone\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mpinecone\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpcsk_3uhm1m_DyXFRsMZxZpf7Q5m1Q7hrN7Fj7s97PpSup2Eo3EryDySx2Eu3z3Yhyywr1XDdZd\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menvironment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mus-east1-gcp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m index_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfurniture-recommendations\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pinecone\u001b[38;5;241m.\u001b[39mlist_indexes():\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/pinecone/deprecation_warnings.py:40\u001b[0m, in \u001b[0;36minit\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m     example \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124m    import os\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124m    from pinecone import Pinecone, ServerlessSpec\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124m        )\u001b[39m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     33\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124minit is no longer a top-level attribute of the pinecone package.\u001b[39m\n\u001b[1;32m     34\u001b[0m \n\u001b[1;32m     35\u001b[0m \u001b[38;5;124mPlease create an instance of the Pinecone class instead.\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;132;01m{\u001b[39;00mexample\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(msg)\n",
      "\u001b[0;31mAttributeError\u001b[0m: init is no longer a top-level attribute of the pinecone package.\n\nPlease create an instance of the Pinecone class instead.\n\nExample:\n\n    import os\n    from pinecone import Pinecone, ServerlessSpec\n\n    pc = Pinecone(\n        api_key=os.environ.get(\"PINECONE_API_KEY\")\n    )\n\n    # Now do stuff\n    if 'my_index' not in pc.list_indexes().names():\n        pc.create_index(\n            name='my_index',\n            dimension=1536,\n            metric='euclidean',\n            spec=ServerlessSpec(\n                cloud='aws',\n                region='us-west-2'\n            )\n        )\n\n"
     ]
    }
   ],
   "source": [
    "!pip install pinecone\n",
    "import pinecone\n",
    "\n",
    "pinecone.init(api_key=\"\", environment=\"us-east1-gcp\")\n",
    "index_name = \"furniture-recommendations\"\n",
    "\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    pinecone.create_index(index_name, dimension=384, metric=\"cosine\")\n",
    "\n",
    "index = pinecone.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "803b216a-9001-40c4-ac79-6fed6026124c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Dataset loaded successfully.\n",
      "Dataset shape: (312, 12)\n",
      "Created 'combined_text' feature.\n",
      "Original dataframe rows: 312\n",
      "Cleaned dataframe rows (with valid image URLs): 312\n",
      "CLIP model and processor loaded.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad1ceaaa94314ebdbf09997bf512c1cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Embeddings:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding generation complete.\n",
      "Shape of final embeddings tensor: torch.Size([312, 1024])\n",
      "Pinecone client initialized.\n",
      "Index 'furniture-recommendations' already exists.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87ea8436391049d3a20d3df3a1d7008e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upserting to Pinecone:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upserting complete.\n",
      "Query: 'a comfortable wooden chair for the living room'\n",
      "\n",
      "  - ID: 0acd8c1c-d689-5fc2-b0ba-9909a60f47dd\n",
      "    Score: 0.6779\n",
      "--------------------\n",
      "  - ID: e0ea5029-8dae-5261-9c57-e98bd40e5bdb\n",
      "    Score: 0.6618\n",
      "--------------------\n",
      "  - ID: 487adf3a-9485-5500-9c98-bcc391eda169\n",
      "    Score: 0.6611\n",
      "--------------------\n",
      "  - ID: 0583ef58-47cd-509b-9e6d-89a0ad8490b2\n",
      "    Score: 0.6560\n",
      "--------------------\n",
      "  - ID: ce921425-0121-53b8-9ce8-1f455be7c9e8\n",
      "    Score: 0.6527\n",
      "--------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "284ce418441148a2aae4ceb66f0d6e8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OSError",
     "evalue": "You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/google/gemma-2b-it.\n401 Client Error. (Request ID: Root=1-68f2c93d-1b3ec6894f41bd5b45693053;ef1492c0-11a1-493d-8a40-f58ea752a741)\n\nCannot access gated repo for url https://huggingface.co/google/gemma-2b-it/resolve/main/config.json.\nAccess to model google/gemma-2b-it is restricted. You must have access to it and be authenticated to access it. Please log in.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_http.py:407\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 407\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/google/gemma-2b-it/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mGatedRepoError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:479\u001b[0m, in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[0;32m--> 479\u001b[0m     \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:1010\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1010\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m   1014\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m   1025\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:1117\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1116\u001b[0m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[0;32m-> 1117\u001b[0m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:1658\u001b[0m, in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1653\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, (RepositoryNotFoundError, GatedRepoError)) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(head_call_error, HfHubHTTPError) \u001b[38;5;129;01mand\u001b[39;00m head_call_error\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m401\u001b[39m\n\u001b[1;32m   1655\u001b[0m ):\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[1;32m   1657\u001b[0m     \u001b[38;5;66;03m# Unauthorized => likely a token issue => let's raise the actual error\u001b[39;00m\n\u001b[0;32m-> 1658\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[1;32m   1659\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1660\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:1546\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1545\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1546\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1547\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\n\u001b[1;32m   1548\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:1463\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers, endpoint)\u001b[0m\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1463\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1465\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1466\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1469\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1470\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1471\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1472\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:286\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 286\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:310\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    309\u001b[0m response \u001b[38;5;241m=\u001b[39m http_backoff(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 310\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_http.py:424\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    421\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    422\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot access gated repo for url \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    423\u001b[0m     )\n\u001b[0;32m--> 424\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(GatedRepoError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m error_message \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccess to this resource is disabled.\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mGatedRepoError\u001b[0m: 401 Client Error. (Request ID: Root=1-68f2c93d-1b3ec6894f41bd5b45693053;ef1492c0-11a1-493d-8a40-f58ea752a741)\n\nCannot access gated repo for url https://huggingface.co/google/gemma-2b-it/resolve/main/config.json.\nAccess to model google/gemma-2b-it is restricted. You must have access to it and be authenticated to access it. Please log in.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 350\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# +\u001b[39;00m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;66;03m# Initialize the LLM pipeline\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;66;03m# Using a smaller, faster model like gemma-2b-it is ideal for this task.\u001b[39;00m\n\u001b[1;32m    349\u001b[0m llm_model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/gemma-2b-it\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 350\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mHuggingFacePipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_model_id\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm_model_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpipeline_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_new_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtorch_dtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Add this to ensure secure loading\u001b[39;49;00m\n\u001b[1;32m    356\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;66;03m# Create a prompt template using LangChain\u001b[39;00m\n\u001b[1;32m    359\u001b[0m prompt_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;124mYou are a creative copywriter for a high-end furniture store.\u001b[39m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;124mWrite a short, engaging, and creative product description based on the following details.\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;124mCreative Description:\u001b[39m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_huggingface/llms/huggingface_pipeline.py:147\u001b[0m, in \u001b[0;36mHuggingFacePipeline.from_model_id\u001b[0;34m(cls, model_id, task, backend, device, device_map, model_kwargs, pipeline_kwargs, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m    146\u001b[0m     _model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice_map\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m device_map\n\u001b[0;32m--> 147\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_model_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenvino\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mipex\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m task \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m VALID_TASKS:\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/tokenization_auto.py:1093\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1091\u001b[0m         config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfor_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_dict)\n\u001b[1;32m   1092\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1093\u001b[0m         config \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1094\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   1095\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1096\u001b[0m config_tokenizer_class \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mtokenizer_class\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mauto_map:\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/configuration_auto.py:1333\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1330\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1331\u001b[0m code_revision \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_revision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1333\u001b[0m config_dict, unused_kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mPretrainedConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1334\u001b[0m has_remote_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1335\u001b[0m has_local_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/configuration_utils.py:662\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    660\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    661\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 662\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    664\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {}, kwargs\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/configuration_utils.py:721\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    717\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_configuration_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, CONFIG_NAME) \u001b[38;5;28;01mif\u001b[39;00m gguf_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m gguf_file\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    720\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    727\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    732\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    736\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, kwargs\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:322\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcached_file\u001b[39m(\n\u001b[1;32m    265\u001b[0m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[1;32m    266\u001b[0m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    268\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    269\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;124;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 322\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m     file \u001b[38;5;241m=\u001b[39m file[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:543\u001b[0m, in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _raise_exceptions_for_gated_repo:\n\u001b[1;32m    542\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 543\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    544\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to have access to it at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    545\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    546\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, LocalEntryNotFoundError):\n\u001b[1;32m    548\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _raise_exceptions_for_connection_errors:\n",
      "\u001b[0;31mOSError\u001b[0m: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/google/gemma-2b-it.\n401 Client Error. (Request ID: Root=1-68f2c93d-1b3ec6894f41bd5b45693053;ef1492c0-11a1-493d-8a40-f58ea752a741)\n\nCannot access gated repo for url https://huggingface.co/google/gemma-2b-it/resolve/main/config.json.\nAccess to model google/gemma-2b-it is restricted. You must have access to it and be authenticated to access it. Please log in."
     ]
    }
   ],
   "source": [
    "# ---\n",
    "# jupyter:\n",
    "#   jupytext:\n",
    "#     text_representation:\n",
    "#       extension: .py\n",
    "#       format_name: light\n",
    "#       format_version: '1.5'\n",
    "#       jupytext_version: 1.16.2\n",
    "#   kernelspec:\n",
    "#     display_name: Python 3\n",
    "#     language: python\n",
    "#     name: python3\n",
    "# ---\n",
    "\n",
    "# # Product Recommendation System: Model Training & Vector DB Setup\n",
    "#\n",
    "# **Objective:** This notebook covers the end-to-end machine learning pipeline for the product recommendation web app.\n",
    "#\n",
    "# **Key Tasks:**\n",
    "# 1.  **Data Loading & Preprocessing:** Load the furniture dataset and clean it for model consumption.\n",
    "# 2.  **Feature Extraction (Embeddings):** Use OpenAI's CLIP model to generate powerful multi-modal (text and image) embeddings for each product.\n",
    "# 3.  **Vector Database Setup:** Initialize and configure a Pinecone vector database to store these embeddings.\n",
    "# 4.  **Upsert to Pinecone:** Populate the Pinecone index with our product embeddings for efficient similarity search.\n",
    "# 5.  **Semantic Search:** Implement and test a function to find similar products based on a text query or image.\n",
    "# 6.  **Generative AI for Descriptions:** Use a lightweight GenAI model via LangChain to create creative product descriptions.\n",
    "#\n",
    "# **Folder Structure Context:**\n",
    "# ```\n",
    "\n",
    "\n",
    "# ## Step 1: Setup and Installations\n",
    "#\n",
    "# First, let's install the necessary libraries for this project. We've added `safetensors` to ensure models are loaded securely and avoid potential `torch.load` errors.\n",
    "\n",
    "# +\n",
    "# !pip install -q -U pandas torch transformers safetensors pinecone-client sentence-transformers pillow requests tqdm langchain langchain-core langchain-huggingface\n",
    "# -\n",
    "\n",
    "# ## Step 2: Import Libraries and Load Data\n",
    "\n",
    "import io\n",
    "import os\n",
    "\n",
    "import ipywidgets.widgets as widgets  # type: ignore\n",
    "\n",
    "# +\n",
    "import pandas as pd\n",
    "import requests\n",
    "import torch\n",
    "from IPython.display import display  # type: ignore\n",
    "from langchain_core.prompts import (\n",
    "    PromptTemplate,  # Updated import for recent langchain versions\n",
    ")\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from PIL import Image\n",
    "from pinecone import Pinecone, ServerlessSpec  # Updated Pinecone import\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "\n",
    "# Check for GPU availability for faster processing\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "# -\n",
    "\n",
    "# Load the dataset. Make sure the path is correct based on the folder structure.\n",
    "try:\n",
    "    df = pd.read_csv('intern_data_ikarus.csv')\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "    print(\"Dataset shape:\", df.shape)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Dataset file not found. Please ensure 'intern_data_ikarus.csv' is in the same directory as the notebook or provide the correct path.\")\n",
    "\n",
    "df.head()\n",
    "\n",
    "# ## Step 3: Data Cleaning and Preprocessing\n",
    "#\n",
    "# We need to prepare the data for our models. This involves handling missing values and creating a unified text field for each product that will be used to generate text embeddings. We also need to clean the image URLs.\n",
    "\n",
    "# +\n",
    "# --- Handle Missing Values ---\n",
    "# Fill missing values in key text columns with an empty string or a placeholder.\n",
    "# This ensures that our combined text field doesn't fail on NaN values.\n",
    "text_cols = ['title', 'description', 'brand', 'categories', 'material', 'color']\n",
    "for col in text_cols:\n",
    "    df[col] = df[col].fillna('')\n",
    "\n",
    "# --- Create a Combined Text Feature ---\n",
    "# We create a single, rich text description for each product. This helps the model\n",
    "# understand the product from multiple textual attributes simultaneously.\n",
    "def combine_text_features(row):\n",
    "    return (\n",
    "        f\"Title: {row['title']}. \"\n",
    "        f\"Brand: {row['brand']}. \"\n",
    "        f\"Categories: {row['categories']}. \"\n",
    "        f\"Description: {row['description']}. \"\n",
    "        f\"Material: {row['material']}. \"\n",
    "        f\"Color: {row['color']}.\"\n",
    "    )\n",
    "\n",
    "df['combined_text'] = df.apply(combine_text_features, axis=1)\n",
    "\n",
    "print(\"Created 'combined_text' feature.\")\n",
    "df[['uniq_id', 'combined_text']].head()\n",
    "# -\n",
    "\n",
    "# --- Clean Image URLs ---\n",
    "# The 'images' column is a string representation of a list. We need to parse it\n",
    "# and extract the first valid image URL.\n",
    "def extract_first_image_url(images_str):\n",
    "    try:\n",
    "        # The string looks like \"['url1', 'url2', ...]\". We strip brackets and quotes.\n",
    "        image_list = images_str.strip(\"[]\").split(\"', '\")\n",
    "        first_image = image_list[0].strip(\"'\")\n",
    "        return first_image if first_image else None\n",
    "    except (IndexError, AttributeError):\n",
    "        return None\n",
    "\n",
    "df['main_image_url'] = df['images'].apply(extract_first_image_url)\n",
    "\n",
    "# Drop rows where we couldn't find a valid image URL, as they can't be processed by CLIP.\n",
    "df_clean = df.dropna(subset=['main_image_url']).copy()\n",
    "df_clean = df_clean.reset_index(drop=True)\n",
    "\n",
    "print(f\"Original dataframe rows: {len(df)}\")\n",
    "print(f\"Cleaned dataframe rows (with valid image URLs): {len(df_clean)}\")\n",
    "df_clean[['uniq_id', 'main_image_url']].head()\n",
    "\n",
    "\n",
    "# ## Step 4: Initialize CLIP Model\n",
    "#\n",
    "# We'll use the `openai/clip-vit-base-patch32` model. This model is a powerful multi-modal encoder that can process both text and images into the same embedding space, making it perfect for our use case.\n",
    "\n",
    "# +\n",
    "model_name = \"openai/clip-vit-base-patch32\"\n",
    "processor = CLIPProcessor.from_pretrained(model_name)\n",
    "# Use `use_safetensors=True` to avoid the torch.load vulnerability error with older torch versions.\n",
    "model = CLIPModel.from_pretrained(model_name, use_safetensors=True).to(device)\n",
    "\n",
    "print(\"CLIP model and processor loaded.\")\n",
    "# -\n",
    "\n",
    "# ## Step 5: Generate Embeddings\n",
    "#\n",
    "# Now, we'll process our entire dataset. For each product, we'll generate a text embedding, an image embedding, and then combine them.\n",
    "\n",
    "# ### Helper function to download and process images\n",
    "def get_image_from_url(url):\n",
    "    \"\"\"Downloads an image from a URL and returns a PIL Image.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        response.raise_for_status()\n",
    "        image = Image.open(io.BytesIO(response.content)).convert(\"RGB\")\n",
    "        return image\n",
    "    except Exception as e:\n",
    "        # print(f\"Warning: Could not download or process image from {url}. Error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# ### Generate Text and Image Embeddings in Batches\n",
    "# We process in batches to manage memory usage, especially if using a GPU.\n",
    "\n",
    "# +\n",
    "batch_size = 64\n",
    "all_combined_embeddings = []\n",
    "valid_ids = []\n",
    "\n",
    "# We'll iterate through the dataframe in chunks (batches)\n",
    "for i in tqdm(range(0, len(df_clean), batch_size), desc=\"Generating Embeddings\"):\n",
    "    batch_df = df_clean.iloc[i:i+batch_size]\n",
    "    \n",
    "    # --- Process Texts ---\n",
    "    texts = batch_df['combined_text'].tolist()\n",
    "    text_inputs = processor(text=texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=77).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features = model.get_text_features(**text_inputs)\n",
    "        \n",
    "    # --- Process Images ---\n",
    "    image_urls = batch_df['main_image_url'].tolist()\n",
    "    images = [get_image_from_url(url) for url in image_urls]\n",
    "    \n",
    "    # Filter out any images that failed to download\n",
    "    valid_images = [img for img in images if img is not None]\n",
    "    valid_indices = [idx for idx, img in enumerate(images) if img is not None]\n",
    "\n",
    "    if not valid_images:\n",
    "        continue # Skip batch if no images are valid\n",
    "        \n",
    "    image_inputs = processor(images=valid_images, return_tensors=\"pt\", padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        image_features_valid = model.get_image_features(**image_inputs)\n",
    "    \n",
    "    # Create a tensor for all images in the batch, filling failed ones with zeros\n",
    "    image_features = torch.zeros(len(batch_df), image_features_valid.shape[1]).to(device)\n",
    "    image_features[valid_indices, :] = image_features_valid\n",
    "\n",
    "    # --- Combine and Normalize Embeddings ---\n",
    "    # Concatenate text and image features to create a multi-modal embedding\n",
    "    combined_features = torch.cat((text_features, image_features), dim=1)\n",
    "    \n",
    "    # Normalize the embeddings to have a unit length (magnitude of 1).\n",
    "    # This is a best practice for similarity search with cosine distance.\n",
    "    normalized_features = combined_features / combined_features.norm(dim=1, keepdim=True)\n",
    "    \n",
    "    all_combined_embeddings.append(normalized_features.cpu())\n",
    "    valid_ids.extend(batch_df['uniq_id'].tolist())\n",
    "\n",
    "# Concatenate all batch results into a single tensor\n",
    "final_embeddings = torch.cat(all_combined_embeddings, dim=0)\n",
    "\n",
    "print(\"Embedding generation complete.\")\n",
    "print(\"Shape of final embeddings tensor:\", final_embeddings.shape)\n",
    "# -\n",
    "\n",
    "# ## Step 6: Setup Pinecone Vector Database (Updated Syntax)\n",
    "#\n",
    "# **Action Required:** You need to sign up for a free Pinecone account at [pinecone.io](https://www.pinecone.io/) and get your API key.\n",
    "\n",
    "# +\n",
    "# --- Pinecone Initialization ---\n",
    "# PLEASE REPLACE with your actual Pinecone API key\n",
    "PINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\", \")\n",
    "\n",
    "if PINECONE_API_KEY == \"YOUR_API_KEY\":\n",
    "    print(\"🚨 Please replace 'YOUR_API_KEY' with your actual Pinecone API key.\")\n",
    "else:\n",
    "    # New initialization method\n",
    "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "    print(\"Pinecone client initialized.\")\n",
    "\n",
    "# -\n",
    "\n",
    "# --- Create a Pinecone Index ---\n",
    "# An index is where our vectors will be stored. We define its name, dimension, and spec.\n",
    "index_name = \"furniture-recommendations\"\n",
    "embedding_dim = final_embeddings.shape[1]\n",
    "\n",
    "# New method to check if an index exists\n",
    "existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
    "\n",
    "if index_name not in existing_indexes:\n",
    "    print(f\"Creating index '{index_name}'...\")\n",
    "    # New method to create an index\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=embedding_dim,\n",
    "        metric='cosine',\n",
    "        spec=ServerlessSpec(\n",
    "            cloud='aws',\n",
    "            region='us-east-1' # You can choose a region that is close to you\n",
    "        )\n",
    "    )\n",
    "    print(\"Index created successfully.\")\n",
    "else:\n",
    "    print(f\"Index '{index_name}' already exists.\")\n",
    "\n",
    "# Connect to the index (this is also updated)\n",
    "index = pc.Index(index_name)\n",
    "index.describe_index_stats()\n",
    "\n",
    "# ## Step 7: Upsert Embeddings to Pinecone\n",
    "#\n",
    "# Now we'll upload our generated embeddings into the Pinecone index. We'll do this in batches for efficiency. We will also store the `combined_text` as metadata.\n",
    "\n",
    "# +\n",
    "# We need to associate each vector with its unique product ID and text.\n",
    "vectors_to_upsert = []\n",
    "for i, (uniq_id, embedding) in enumerate(zip(valid_ids, final_embeddings)):\n",
    "    # Pinecone expects a list of tuples: (id, vector, metadata)\n",
    "    metadata = {'text': df_clean.loc[i, 'combined_text']}\n",
    "    vectors_to_upsert.append((uniq_id, embedding.tolist(), metadata))\n",
    "\n",
    "# Upsert in batches to avoid overwhelming the API\n",
    "upsert_batch_size = 100\n",
    "for i in tqdm(range(0, len(vectors_to_upsert), upsert_batch_size), desc=\"Upserting to Pinecone\"):\n",
    "    batch = vectors_to_upsert[i:i+upsert_batch_size]\n",
    "    index.upsert(vectors=batch)\n",
    "\n",
    "print(\"Upserting complete.\")\n",
    "index.describe_index_stats()\n",
    "# -\n",
    "\n",
    "# ## Step 8: Implement Semantic Search\n",
    "#\n",
    "# Let's test our system! We'll create a function that takes a text query, generates a combined embedding for it (we'll use a blank image for a pure text query), and searches Pinecone.\n",
    "\n",
    "# +\n",
    "def find_similar_products(query_text, top_k=5):\n",
    "    \"\"\"\n",
    "    Finds similar products in the Pinecone index based on a text query.\n",
    "    \"\"\"\n",
    "    # 1. Generate text embedding for the query\n",
    "    text_inputs = processor(text=[query_text], return_tensors=\"pt\", padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        query_text_embedding = model.get_text_features(**text_inputs)\n",
    "        \n",
    "    # 2. Create a \"zero\" image embedding as we are only searching with text\n",
    "    # CORRECTED: Use `model.config.projection_dim` which is the final output dimension (512)\n",
    "    # for both text and image embeddings, ensuring the dimensions match.\n",
    "    # The original error was caused by using a different internal dimension.\n",
    "    query_image_embedding = torch.zeros(1, model.config.projection_dim).to(device)\n",
    "    \n",
    "    # 3. Combine and normalize\n",
    "    query_embedding = torch.cat((query_text_embedding, query_image_embedding), dim=1)\n",
    "    query_embedding = query_embedding / query_embedding.norm(dim=1, keepdim=True)\n",
    "    \n",
    "    # 4. Query Pinecone\n",
    "    results = index.query(\n",
    "        vector=query_embedding.cpu().tolist(),\n",
    "        top_k=top_k,\n",
    "        include_metadata=True\n",
    "    )\n",
    "    \n",
    "    return results['matches']\n",
    "\n",
    "# --- Let's test it! ---\n",
    "query = \"a comfortable wooden chair for the living room\"\n",
    "matches = find_similar_products(query)\n",
    "\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "for match in matches:\n",
    "    print(f\"  - ID: {match['id']}\")\n",
    "    print(f\"    Score: {match['score']:.4f}\")\n",
    "    # print(f\"    Text: {match['metadata']['text'][:150]}...\")\n",
    "    print(\"-\" * 20)\n",
    "# -\n",
    "\n",
    "# ## Step 9: Generative AI for Creative Descriptions\n",
    "#\n",
    "# As per the assignment, we need to use a GenAI model to generate creative descriptions for the recommended products. We'll use LangChain for this.\n",
    "#\n",
    "# **Note:** We'll use a lightweight, open-source model from HuggingFace to avoid API costs. `google/gemma-2b-it` is a great choice. You may need to log in to HuggingFace for access.\n",
    "\n",
    "# +\n",
    "# from huggingface_hub import notebook_login\n",
    "\n",
    "# notebook_login() # Run this if you need to authenticate with HuggingFace\n",
    "\n",
    "# +\n",
    "# Initialize the LLM pipeline\n",
    "# Using a smaller, faster model like gemma-2b-it is ideal for this task.\n",
    "llm_model_name = \"google/gemma-2b-it\"\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=llm_model_name,\n",
    "    task=\"text-generation\",\n",
    "    device=device,\n",
    "    pipeline_kwargs={\"max_new_tokens\": 100, \"torch_dtype\":torch.bfloat16},\n",
    "    use_safetensors=True # Add this to ensure secure loading\n",
    ")\n",
    "\n",
    "# Create a prompt template using LangChain\n",
    "prompt_template = \"\"\"\n",
    "You are a creative copywriter for a high-end furniture store.\n",
    "Write a short, engaging, and creative product description based on the following details.\n",
    "Do not just repeat the details, but weave them into a compelling narrative.\n",
    "\n",
    "Product Details:\n",
    "- Title: {title}\n",
    "- Brand: {brand}\n",
    "- Material: {material}\n",
    "- Color: {color}\n",
    "\n",
    "Creative Description:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"title\", \"brand\", \"material\", \"color\"])\n",
    "\n",
    "# Create the LangChain chain using the modern LCEL (LangChain Expression Language) pipe syntax.\n",
    "# This is more robust with recent langchain versions.\n",
    "chain = prompt | llm\n",
    "\n",
    "print(\"LangChain with Gemma model initialized.\")\n",
    "\n",
    "\n",
    "# -\n",
    "\n",
    "# ### Function to get product details and generate new description\n",
    "def generate_creative_description(product_id):\n",
    "    \"\"\"\n",
    "    Fetches product details and uses the LLM to generate a new description.\n",
    "    \"\"\"\n",
    "    # Find the product details from our original dataframe\n",
    "    product_details = df_clean[df_clean['uniq_id'] == product_id].iloc[0]\n",
    "    \n",
    "    # The input for the chain is a dictionary with keys matching the prompt's input variables\n",
    "    input_data = {\n",
    "        'title': product_details['title'],\n",
    "        'brand': product_details['brand'],\n",
    "        'material': product_details['material'],\n",
    "        'color': product_details['color']\n",
    "    }\n",
    "    \n",
    "    # Run the chain. The result of a simple `prompt | llm` chain is the generated string.\n",
    "    creative_description = chain.invoke(input_data)\n",
    "    \n",
    "    return creative_description\n",
    "\n",
    "\n",
    "# --- Test the full recommendation-to-generation pipeline ---\n",
    "print(\"--- Testing the Full Pipeline ---\")\n",
    "query = \"a stylish metal table for outdoor dining\"\n",
    "search_results = find_similar_products(query, top_k=2)\n",
    "\n",
    "for result in search_results:\n",
    "    product_id = result['id']\n",
    "    print(f\"\\nProduct ID: {product_id}\")\n",
    "    print(f\"Similarity Score: {result['score']:.4f}\")\n",
    "    \n",
    "    # Generate a new description\n",
    "    creative_desc = generate_creative_description(product_id)\n",
    "    print(\"\\nGenerated Creative Description:\")\n",
    "    print(creative_desc)\n",
    "    print(\"=\"*30)\n",
    "\n",
    "# ## Conclusion\n",
    "#\n",
    "# This notebook has successfully:\n",
    "# 1. Processed and cleaned the furniture dataset.\n",
    "# 2. Generated multi-modal embeddings using the CLIP model.\n",
    "# 3. Set up a Pinecone index and populated it with the product embeddings.\n",
    "# 4. Implemented a semantic search function to find relevant products.\n",
    "# 5. Integrated a generative AI model using LangChain to create compelling product descriptions.\n",
    "#\n",
    "# The assets and logic created here are now ready to be integrated into the FastAPI backend. The `find_similar_products` and `generate_creative_description` functions form the core logic for your API endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33f4a352-fc6e-46c3-9843-5cf97c2256ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Dataset loaded successfully.\n",
      "Dataset shape: (312, 12)\n",
      "Created 'combined_text' feature.\n",
      "Original dataframe rows: 312\n",
      "Cleaned dataframe rows (with valid image URLs): 312\n",
      "CLIP model and processor loaded.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3252f2ca8ea645139572f6a2b5cafebb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Embeddings:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding generation complete.\n",
      "Shape of final embeddings tensor: torch.Size([312, 1024])\n",
      "Pinecone client initialized.\n",
      "Index 'furniture-recommendations' already exists.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d49d9e2e4e0447d48ee77d8356c6c8bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upserting to Pinecone:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upserting complete.\n",
      "Query: 'a comfortable wooden chair for the living room'\n",
      "\n",
      "  - ID: 0acd8c1c-d689-5fc2-b0ba-9909a60f47dd\n",
      "    Score: 0.6779\n",
      "--------------------\n",
      "  - ID: e0ea5029-8dae-5261-9c57-e98bd40e5bdb\n",
      "    Score: 0.6618\n",
      "--------------------\n",
      "  - ID: 487adf3a-9485-5500-9c98-bcc391eda169\n",
      "    Score: 0.6611\n",
      "--------------------\n",
      "  - ID: 0583ef58-47cd-509b-9e6d-89a0ad8490b2\n",
      "    Score: 0.6560\n",
      "--------------------\n",
      "  - ID: ce921425-0121-53b8-9ce8-1f455be7c9e8\n",
      "    Score: 0.6527\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain with TinyLlama model initialized.\n",
      "--- Testing the Full Pipeline ---\n",
      "\n",
      "Product ID: 9e82e445-4d8d-5fd1-9dcf-a92d9783365c\n",
      "Similarity Score: 0.6438\n",
      "\n",
      "Generated Creative Description:\n",
      "\n",
      "You are a creative copywriter for a high-end furniture store.\n",
      "Write a short, engaging, and creative product description based on the following details.\n",
      "Do not just repeat the details, but weave them into a compelling narrative.\n",
      "\n",
      "Product Details:\n",
      "- Title: Need Fold Wall Mounted Workbench Folding Wall Table Length 47.2\" Width 20\" Perfect Addition to Garage & Shed/Home Office/Laundry Room/Home Bar/Kitchen & Dining Room\n",
      "- Brand: Need Store\n",
      "- Material: Metal\n",
      "- Color: Teak Color Desktop & Warm White Folding Brackets\n",
      "\n",
      "Creative Description:\n",
      "The Need Fold Wall Mounted Workbench Folding Wall Table is an excellent addition to any garage, shed, home office, laundry room, home bar, kitchen, and dining room. Its sleek design is perfect for those who love to work in a compact space. The table folds flat for easy storage when not in use, making it a perfect addition when you don't have much space.\n",
      "\n",
      "The folding wall table comes with two metal bra\n",
      "==============================\n",
      "\n",
      "Product ID: 487adf3a-9485-5500-9c98-bcc391eda169\n",
      "Similarity Score: 0.6351\n",
      "\n",
      "Generated Creative Description:\n",
      "\n",
      "You are a creative copywriter for a high-end furniture store.\n",
      "Write a short, engaging, and creative product description based on the following details.\n",
      "Do not just repeat the details, but weave them into a compelling narrative.\n",
      "\n",
      "Product Details:\n",
      "- Title: 3-Tier Side Table,Narrow End Table with Storage Shelf,Minimalist Bedside Tables Nightstand,Small Bookshelf Bookcase for Spaces,Bathroom Shelve,Display Rack for Bedroom,Living Room,Office,Dorms,2 Pack\n",
      "- Brand: HomeToDou\n",
      "- Material: Engineered Wood\n",
      "- Color: White\n",
      "\n",
      "Creative Description:\n",
      "This stunning 3-tier side table and narrow end table with storage shelf is the perfect combination of sophistication and style. With its sleek and minimalist design, this piece is perfect for any decor scheme. Its narrow end table with storage shelf adds extra functionality to any space, making it ideal for storing books, magazines, or any small items. The multi-functional design of this piece makes it a perfect addition to any home or office space.\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "# ---\n",
    "# jupyter:\n",
    "#   jupytext:\n",
    "#     text_representation:\n",
    "#       extension: .py\n",
    "#       format_name: light\n",
    "#       format_version: '1.5'\n",
    "#       jupytext_version: 1.16.2\n",
    "#   kernelspec:\n",
    "#     display_name: Python 3\n",
    "#     language: python\n",
    "#     name: python3\n",
    "# ---\n",
    "\n",
    "# # Product Recommendation System: Model Training & Vector DB Setup\n",
    "#\n",
    "# **Objective:** This notebook covers the end-to-end machine learning pipeline for the product recommendation web app.\n",
    "#\n",
    "# **Key Tasks:**\n",
    "# 1.  **Data Loading & Preprocessing:** Load the furniture dataset and clean it for model consumption.\n",
    "# 2.  **Feature Extraction (Embeddings):** Use OpenAI's CLIP model to generate powerful multi-modal (text and image) embeddings for each product.\n",
    "# 3.  **Vector Database Setup:** Initialize and configure a Pinecone vector database to store these embeddings.\n",
    "# 4.  **Upsert to Pinecone:** Populate the Pinecone index with our product embeddings for efficient similarity search.\n",
    "# 5.  **Semantic Search:** Implement and test a function to find similar products based on a text query or image.\n",
    "# 6.  **Generative AI for Descriptions:** Use a lightweight GenAI model via LangChain to create creative product descriptions.\n",
    "#\n",
    "# **Folder Structure Context:**\n",
    "# ```\n",
    "# AI-ML-Assignment/\n",
    "# │\n",
    "# ├── notebooks/\n",
    "# │   ├── model_training.ipynb  <-- YOU ARE HERE\n",
    "# │   └── analytics.ipynb\n",
    "# │\n",
    "# ├── data/\n",
    "# │   └── intern_data_ikarus.csv\n",
    "# ```\n",
    "\n",
    "# ## Step 1: Setup and Installations\n",
    "#\n",
    "# First, let's install the necessary libraries for this project. We've added `safetensors` to ensure models are loaded securely and avoid potential `torch.load` errors.\n",
    "\n",
    "# +\n",
    "# !pip install -q -U pandas torch transformers safetensors pinecone-client sentence-transformers pillow requests tqdm langchain langchain-core langchain-huggingface\n",
    "# -\n",
    "\n",
    "# ## Step 2: Import Libraries and Load Data\n",
    "\n",
    "# +\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import requests\n",
    "import io\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "from pinecone import Pinecone, ServerlessSpec # Updated Pinecone import\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain_core.prompts import PromptTemplate # Updated import for recent langchain versions\n",
    "\n",
    "# Check for GPU availability for faster processing\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "# -\n",
    "\n",
    "# Load the dataset. Make sure the path is correct based on the folder structure.\n",
    "try:\n",
    "    df = pd.read_csv('intern_data_ikarus.csv')\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "    print(\"Dataset shape:\", df.shape)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Dataset file not found. Please ensure 'intern_data_ikarus.csv' is in the same directory as the notebook or provide the correct path.\")\n",
    "\n",
    "df.head()\n",
    "\n",
    "# ## Step 3: Data Cleaning and Preprocessing\n",
    "#\n",
    "# We need to prepare the data for our models. This involves handling missing values and creating a unified text field for each product that will be used to generate text embeddings. We also need to clean the image URLs.\n",
    "\n",
    "# +\n",
    "# --- Handle Missing Values ---\n",
    "# Fill missing values in key text columns with an empty string or a placeholder.\n",
    "# This ensures that our combined text field doesn't fail on NaN values.\n",
    "text_cols = ['title', 'description', 'brand', 'categories', 'material', 'color']\n",
    "for col in text_cols:\n",
    "    df[col] = df[col].fillna('')\n",
    "\n",
    "# --- Create a Combined Text Feature ---\n",
    "# We create a single, rich text description for each product. This helps the model\n",
    "# understand the product from multiple textual attributes simultaneously.\n",
    "def combine_text_features(row):\n",
    "    return (\n",
    "        f\"Title: {row['title']}. \"\n",
    "        f\"Brand: {row['brand']}. \"\n",
    "        f\"Categories: {row['categories']}. \"\n",
    "        f\"Description: {row['description']}. \"\n",
    "        f\"Material: {row['material']}. \"\n",
    "        f\"Color: {row['color']}.\"\n",
    "    )\n",
    "\n",
    "df['combined_text'] = df.apply(combine_text_features, axis=1)\n",
    "\n",
    "print(\"Created 'combined_text' feature.\")\n",
    "df[['uniq_id', 'combined_text']].head()\n",
    "# -\n",
    "\n",
    "# --- Clean Image URLs ---\n",
    "# The 'images' column is a string representation of a list. We need to parse it\n",
    "# and extract the first valid image URL.\n",
    "def extract_first_image_url(images_str):\n",
    "    try:\n",
    "        # The string looks like \"['url1', 'url2', ...]\". We strip brackets and quotes.\n",
    "        image_list = images_str.strip(\"[]\").split(\"', '\")\n",
    "        first_image = image_list[0].strip(\"'\")\n",
    "        return first_image if first_image else None\n",
    "    except (IndexError, AttributeError):\n",
    "        return None\n",
    "\n",
    "df['main_image_url'] = df['images'].apply(extract_first_image_url)\n",
    "\n",
    "# Drop rows where we couldn't find a valid image URL, as they can't be processed by CLIP.\n",
    "df_clean = df.dropna(subset=['main_image_url']).copy()\n",
    "df_clean = df_clean.reset_index(drop=True)\n",
    "\n",
    "print(f\"Original dataframe rows: {len(df)}\")\n",
    "print(f\"Cleaned dataframe rows (with valid image URLs): {len(df_clean)}\")\n",
    "df_clean[['uniq_id', 'main_image_url']].head()\n",
    "\n",
    "\n",
    "# ## Step 4: Initialize CLIP Model\n",
    "#\n",
    "# We'll use the `openai/clip-vit-base-patch32` model. This model is a powerful multi-modal encoder that can process both text and images into the same embedding space, making it perfect for our use case.\n",
    "\n",
    "# +\n",
    "model_name = \"openai/clip-vit-base-patch32\"\n",
    "processor = CLIPProcessor.from_pretrained(model_name)\n",
    "# Use `use_safetensors=True` to avoid the torch.load vulnerability error with older torch versions.\n",
    "model = CLIPModel.from_pretrained(model_name, use_safetensors=True).to(device)\n",
    "\n",
    "print(\"CLIP model and processor loaded.\")\n",
    "# -\n",
    "\n",
    "# ## Step 5: Generate Embeddings\n",
    "#\n",
    "# Now, we'll process our entire dataset. For each product, we'll generate a text embedding, an image embedding, and then combine them.\n",
    "\n",
    "# ### Helper function to download and process images\n",
    "def get_image_from_url(url):\n",
    "    \"\"\"Downloads an image from a URL and returns a PIL Image.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        response.raise_for_status()\n",
    "        image = Image.open(io.BytesIO(response.content)).convert(\"RGB\")\n",
    "        return image\n",
    "    except Exception as e:\n",
    "        # print(f\"Warning: Could not download or process image from {url}. Error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# ### Generate Text and Image Embeddings in Batches\n",
    "# We process in batches to manage memory usage, especially if using a GPU.\n",
    "\n",
    "# +\n",
    "batch_size = 64\n",
    "all_combined_embeddings = []\n",
    "valid_ids = []\n",
    "\n",
    "# We'll iterate through the dataframe in chunks (batches)\n",
    "for i in tqdm(range(0, len(df_clean), batch_size), desc=\"Generating Embeddings\"):\n",
    "    batch_df = df_clean.iloc[i:i+batch_size]\n",
    "    \n",
    "    # --- Process Texts ---\n",
    "    texts = batch_df['combined_text'].tolist()\n",
    "    text_inputs = processor(text=texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=77).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features = model.get_text_features(**text_inputs)\n",
    "        \n",
    "    # --- Process Images ---\n",
    "    image_urls = batch_df['main_image_url'].tolist()\n",
    "    images = [get_image_from_url(url) for url in image_urls]\n",
    "    \n",
    "    # Filter out any images that failed to download\n",
    "    valid_images = [img for img in images if img is not None]\n",
    "    valid_indices = [idx for idx, img in enumerate(images) if img is not None]\n",
    "\n",
    "    if not valid_images:\n",
    "        continue # Skip batch if no images are valid\n",
    "        \n",
    "    image_inputs = processor(images=valid_images, return_tensors=\"pt\", padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        image_features_valid = model.get_image_features(**image_inputs)\n",
    "    \n",
    "    # Create a tensor for all images in the batch, filling failed ones with zeros\n",
    "    image_features = torch.zeros(len(batch_df), image_features_valid.shape[1]).to(device)\n",
    "    image_features[valid_indices, :] = image_features_valid\n",
    "\n",
    "    # --- Combine and Normalize Embeddings ---\n",
    "    # Concatenate text and image features to create a multi-modal embedding\n",
    "    combined_features = torch.cat((text_features, image_features), dim=1)\n",
    "    \n",
    "    # Normalize the embeddings to have a unit length (magnitude of 1).\n",
    "    # This is a best practice for similarity search with cosine distance.\n",
    "    normalized_features = combined_features / combined_features.norm(dim=1, keepdim=True)\n",
    "    \n",
    "    all_combined_embeddings.append(normalized_features.cpu())\n",
    "    valid_ids.extend(batch_df['uniq_id'].tolist())\n",
    "\n",
    "# Concatenate all batch results into a single tensor\n",
    "final_embeddings = torch.cat(all_combined_embeddings, dim=0)\n",
    "\n",
    "print(\"Embedding generation complete.\")\n",
    "print(\"Shape of final embeddings tensor:\", final_embeddings.shape)\n",
    "# -\n",
    "\n",
    "# ## Step 6: Setup Pinecone Vector Database (Updated Syntax)\n",
    "#\n",
    "# **Action Required:** You need to sign up for a free Pinecone account at [pinecone.io](https://www.pinecone.io/) and get your API key.\n",
    "\n",
    "# +\n",
    "# --- Pinecone Initialization ---\n",
    "# PLEASE REPLACE with your actual Pinecone API key\n",
    "PINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\", \"pcsk_3uhm1m_DyXFRsMZxZpf7Q5m1Q7hrN7Fj7s97PpSup2Eo3EryDySx2Eu3z3Yhyywr1XDdZd\")\n",
    "\n",
    "if PINECONE_API_KEY == \"YOUR_API_KEY\":\n",
    "    print(\"🚨 Please replace 'YOUR_API_KEY' with your actual Pinecone API key.\")\n",
    "else:\n",
    "    # New initialization method\n",
    "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "    print(\"Pinecone client initialized.\")\n",
    "\n",
    "# -\n",
    "\n",
    "# --- Create a Pinecone Index ---\n",
    "# An index is where our vectors will be stored. We define its name, dimension, and spec.\n",
    "index_name = \"furniture-recommendations\"\n",
    "embedding_dim = final_embeddings.shape[1]\n",
    "\n",
    "# New method to check if an index exists\n",
    "existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
    "\n",
    "if index_name not in existing_indexes:\n",
    "    print(f\"Creating index '{index_name}'...\")\n",
    "    # New method to create an index\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=embedding_dim,\n",
    "        metric='cosine',\n",
    "        spec=ServerlessSpec(\n",
    "            cloud='aws',\n",
    "            region='us-east-1' # You can choose a region that is close to you\n",
    "        )\n",
    "    )\n",
    "    print(\"Index created successfully.\")\n",
    "else:\n",
    "    print(f\"Index '{index_name}' already exists.\")\n",
    "\n",
    "# Connect to the index (this is also updated)\n",
    "index = pc.Index(index_name)\n",
    "index.describe_index_stats()\n",
    "\n",
    "# ## Step 7: Upsert Embeddings to Pinecone\n",
    "#\n",
    "# Now we'll upload our generated embeddings into the Pinecone index. We'll do this in batches for efficiency. We will also store the `combined_text` as metadata.\n",
    "\n",
    "# +\n",
    "# We need to associate each vector with its unique product ID and text.\n",
    "vectors_to_upsert = []\n",
    "for i, (uniq_id, embedding) in enumerate(zip(valid_ids, final_embeddings)):\n",
    "    # Pinecone expects a list of tuples: (id, vector, metadata)\n",
    "    metadata = {'text': df_clean.loc[i, 'combined_text']}\n",
    "    vectors_to_upsert.append((uniq_id, embedding.tolist(), metadata))\n",
    "\n",
    "# Upsert in batches to avoid overwhelming the API\n",
    "upsert_batch_size = 100\n",
    "for i in tqdm(range(0, len(vectors_to_upsert), upsert_batch_size), desc=\"Upserting to Pinecone\"):\n",
    "    batch = vectors_to_upsert[i:i+upsert_batch_size]\n",
    "    index.upsert(vectors=batch)\n",
    "\n",
    "print(\"Upserting complete.\")\n",
    "index.describe_index_stats()\n",
    "# -\n",
    "\n",
    "# ## Step 8: Implement Semantic Search\n",
    "#\n",
    "# Let's test our system! We'll create a function that takes a text query, generates a combined embedding for it (we'll use a blank image for a pure text query), and searches Pinecone.\n",
    "\n",
    "# +\n",
    "def find_similar_products(query_text, top_k=5):\n",
    "    \"\"\"\n",
    "    Finds similar products in the Pinecone index based on a text query.\n",
    "    \"\"\"\n",
    "    # 1. Generate text embedding for the query\n",
    "    text_inputs = processor(text=[query_text], return_tensors=\"pt\", padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        query_text_embedding = model.get_text_features(**text_inputs)\n",
    "        \n",
    "    # 2. Create a \"zero\" image embedding as we are only searching with text\n",
    "    # CORRECTED: Use `model.config.projection_dim` which is the final output dimension (512)\n",
    "    # for both text and image embeddings, ensuring the dimensions match.\n",
    "    # The original error was caused by using a different internal dimension.\n",
    "    query_image_embedding = torch.zeros(1, model.config.projection_dim).to(device)\n",
    "    \n",
    "    # 3. Combine and normalize\n",
    "    query_embedding = torch.cat((query_text_embedding, query_image_embedding), dim=1)\n",
    "    query_embedding = query_embedding / query_embedding.norm(dim=1, keepdim=True)\n",
    "    \n",
    "    # 4. Query Pinecone\n",
    "    results = index.query(\n",
    "        vector=query_embedding.cpu().tolist(),\n",
    "        top_k=top_k,\n",
    "        include_metadata=True\n",
    "    )\n",
    "    \n",
    "    return results['matches']\n",
    "\n",
    "# --- Let's test it! ---\n",
    "query = \"a comfortable wooden chair for the living room\"\n",
    "matches = find_similar_products(query)\n",
    "\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "for match in matches:\n",
    "    print(f\"  - ID: {match['id']}\")\n",
    "    print(f\"    Score: {match['score']:.4f}\")\n",
    "    # print(f\"    Text: {match['metadata']['text'][:150]}...\")\n",
    "    print(\"-\" * 20)\n",
    "# -\n",
    "\n",
    "# ## Step 9: Generative AI for Creative Descriptions\n",
    "#\n",
    "# As per the assignment, we need to use a GenAI model to generate creative descriptions for the recommended products. We'll use LangChain for this.\n",
    "#\n",
    "# **Note:** We are switching to a non-gated model, `TinyLlama/TinyLlama-1.1B-Chat-v1.0`, to avoid the authentication error caused by the gated `google/gemma-2b-it` model. This model is open-access and does not require a Hugging Face token.\n",
    "\n",
    "# +\n",
    "# Initialize the LLM pipeline\n",
    "# Using a smaller, public model like TinyLlama is ideal to avoid authentication issues.\n",
    "llm_model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "# CORRECTED: Arguments for the underlying model (`torch_dtype`) should go in `model_kwargs`,\n",
    "# and arguments for the pipeline itself (`max_new_tokens`) go in `pipeline_kwargs`.\n",
    "# The `use_safetensors` argument that caused the error is removed, as it's not a valid\n",
    "# top-level parameter and is handled by default.\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=llm_model_name,\n",
    "    task=\"text-generation\",\n",
    "    device=0 if device == \"cuda\" else -1,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    pipeline_kwargs={\"max_new_tokens\": 100},\n",
    ")\n",
    "\n",
    "# Create a prompt template using LangChain\n",
    "prompt_template = \"\"\"\n",
    "You are a creative copywriter for a high-end furniture store.\n",
    "Write a short, engaging, and creative product description based on the following details.\n",
    "Do not just repeat the details, but weave them into a compelling narrative.\n",
    "\n",
    "Product Details:\n",
    "- Title: {title}\n",
    "- Brand: {brand}\n",
    "- Material: {material}\n",
    "- Color: {color}\n",
    "\n",
    "Creative Description:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"title\", \"brand\", \"material\", \"color\"])\n",
    "\n",
    "# Create the LangChain chain using the modern LCEL (LangChain Expression Language) pipe syntax.\n",
    "# This is more robust with recent langchain versions.\n",
    "chain = prompt | llm\n",
    "\n",
    "print(\"LangChain with TinyLlama model initialized.\")\n",
    "\n",
    "\n",
    "# -\n",
    "\n",
    "# ### Function to get product details and generate new description\n",
    "def generate_creative_description(product_id):\n",
    "    \"\"\"\n",
    "    Fetches product details and uses the LLM to generate a new description.\n",
    "    \"\"\"\n",
    "    # Find the product details from our original dataframe\n",
    "    product_details = df_clean[df_clean['uniq_id'] == product_id].iloc[0]\n",
    "    \n",
    "    # The input for the chain is a dictionary with keys matching the prompt's input variables\n",
    "    input_data = {\n",
    "        'title': product_details['title'],\n",
    "        'brand': product_details['brand'],\n",
    "        'material': product_details['material'],\n",
    "        'color': product_details['color']\n",
    "    }\n",
    "    \n",
    "    # Run the chain. The result of a simple `prompt | llm` chain is the generated string.\n",
    "    creative_description = chain.invoke(input_data)\n",
    "    \n",
    "    return creative_description\n",
    "\n",
    "\n",
    "# --- Test the full recommendation-to-generation pipeline ---\n",
    "print(\"--- Testing the Full Pipeline ---\")\n",
    "query = \"a stylish metal table for outdoor dining\"\n",
    "search_results = find_similar_products(query, top_k=2)\n",
    "\n",
    "for result in search_results:\n",
    "    product_id = result['id']\n",
    "    print(f\"\\nProduct ID: {product_id}\")\n",
    "    print(f\"Similarity Score: {result['score']:.4f}\")\n",
    "    \n",
    "    # Generate a new description\n",
    "    creative_desc = generate_creative_description(product_id)\n",
    "    print(\"\\nGenerated Creative Description:\")\n",
    "    print(creative_desc)\n",
    "    print(\"=\"*30)\n",
    "\n",
    "# ## Conclusion\n",
    "#\n",
    "# This notebook has successfully:\n",
    "# 1. Processed and cleaned the furniture dataset.\n",
    "# 2. Generated multi-modal embeddings using the CLIP model.\n",
    "# 3. Set up a Pinecone index and populated it with the product embeddings.\n",
    "# 4. Implemented a semantic search function to find relevant products.\n",
    "# 5. Integrated a generative AI model using LangChain to create compelling product descriptions.\n",
    "#\n",
    "# The assets and logic created here are now ready to be integrated into the FastAPI backend. The `find_similar_products` and `generate_creative_description` functions form the core logic for your API endpoints.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729acece-6f74-48e3-b749-3f27a841d54d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
